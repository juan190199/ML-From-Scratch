import warnings

import numpy as np


class QDA(object):
    """
    Quadratic Discriminant Analysis: Classifier with a quadratic decision boundary,
    generated by fitting class conditional densities to the data using Bayes' rule
    The model fits a Gaussian density to each class

    Attributes
    -----------
    * covariance_: list of len n_classes of ndarray of shape (n_features, n_features)
        For each class, gives the covariance matrix estimated using the samples of that class.
        Only present if `store_covariance` is True.

    * means_: ndarray of shape (n_classes, n_features)
        Class-wise means

    * priors_: ndarray of shape (n_classes,)
        Class priors

    * rotations_ : list of len n_classes of ndarray of shape (n_features, n_k)
        For each class k an array of shape (n_features, n_k),
        where 'n_k= min(n_features, number of elements in class k)'
        It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to `V`,
        the matrix of eigenvectors coming from the SVD of `Xk = U S Vt`
        where `Xk` is the centered matrix of samples from class k.

    * scalings_ : list of len n_classes of ndarray of shape (n_k,)
        For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the
        variance in the rotated coordinate system. It corresponds to `S^2 (n_samples - 1)`,
        where `S` is the diagonal matrix of singular values from the SVD of `Xk`,
        where `Xk` is the centered matrix of samples from class k.

    * classes_ : ndarray of shape (n_classes,)
        Unique class labels
    """

    def __init__(self, priors=None, reg_param=0., store_covariance=False, tol=1.0e-4):
        """
        :param priors: ndarray of shape (n_classes, ), default=None
            Class priors. By default, the class priors are inferred from the training data

        :param reg_param: float, default=0.0
            Regularizes the per-class covariance estimates

        :param store_covariance: bool, default=False
            If True, the class covariance matrices are explicitely computed
            and stored in the `self.covariance_` attribute.

        :param tol: float, default=1.0e-4
            Absolute threshold for a singular value to be considered significant
        """
        self.priors = np.asarray(priors) if priors is not None else None
        self.reg_param = reg_param
        self.store_covariance = store_covariance
        self.tol = tol

    def fit(self, X, y):
        """
        Fit the model according to the given design matrix and parameters

        :param X: ndarray of shape (n_samples, n_features)
            Training data

        :param y: ndarray of shape (n_samples, )
            Target data

        :return: self
        """
        self.classes_, y = np.unique(y, return_inverse=True)
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)

        if self.priors is None:
            self.priors_ = np.bincount(y) / float(n_samples)
        else:
            self.priors_ = self.priors

        cov = None
        store_covariance = self.store_covariance
        if store_covariance:
            cov = []
        means = []
        scalings = []
        rotations = []
        # Normalize data
        for idx in range(n_classes):
            Xg = X[y == idx, :]
            meang = Xg.mean(axis=0)
            means.append(meang)
            Xgc = Xg - meang

            # SVD of normalized data
            # Xgc = U * S * V.T => 1/(n-1) * Xgc.T * Xgc = 1/(n-1) * V * (S ** 2) * V.T
            _, S, Vt = np.linalg.svd(Xgc, full_matrices=False)
            rank = np.sum(S > self.tol)
            if rank < n_features:
                warnings.warn('Variables are collinear')
            # Calculate covariance matrix \Sigma_k = 1 / n-1 (VS^2V^T)
            S2 = (S ** 2) / (len(Xg) - 1)
            S2 = ((1 - self.reg_param) * S2) + self.reg_param
            if self.store_covariance or store_covariance:
                cov.append(np.dot(S2 * Vt.T, Vt))
            scalings.append(S2)
            rotations.append(Vt.T)
        if self.store_covariance or store_covariance:
            self.covariance_ = cov
        self.means_ = np.asarray(means)
        self.scalings_ = scalings
        self.rotations_ = rotations
        return self

    def _decision_function(self, X):
        """
        Return log posterior

        :param X: ndarray of shape (n_samples, n_features)
            Test data

        :return: ndarray of shape (n_samples,) or (n_samples, n_classes)
            Decision function values related to each class, per sample.
            In the two-class case, the shape is (n_samples,), giving the log likelihood ratio of the positive class.
        """
        norm2 = []
        for i in range(len(self.classes_)):
            R = self.rotations_[i]
            S = self.scalings_[i]
            Xm = X - self.means_[i]
            X2 = np.dot(Xm, R * (S ** (-0.5)))
            norm2.append(np.sum(X2 ** 2, axis=1))
        norm2 = np.array(norm2).T  # Shape (X.shape[0], n_classes)
        u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])
        return -0.5 * (norm2 + u) + np.log(self.priors_)

    def decision_function(self, X):
        """
        Apply decision function to an array of samples.
        The decision function is equal (up to a constant factor) to the log-posterior of the model,
        i.e. `log p(y = k | x)`. In a binary classification setting this instead corresponds to the difference
        `log p(y = 1 | x) - log p(y = 0 | x)`.

        :param X: ndarray of shape (n_samples, n_features)
            Test data

        :return: ndarray of shape (n_samples,) or (n_samples, n_classes)
            Decision function values related to each class, per sample.
            In the two-class case, the shape is (n_samples,), giving the log likelihood ratio of the positive class.
        """
        dec_function = self._decision_function(X)
        # Handle special case of two classes
        if len(self.classes_) == 2:
            return dec_function[:, 1] - dec_function[:, 0]
        return dec_function

    def predict(self, X):
        """
        Perform classification on an array of test vectors X.

        :param X: ndarray of shape (n_samples, n_features)
            Test data

        :return: ndarray of shape (n_samples,)
            Vector of predicted labels for each sample
        """
        d = self._decision_function(X)
        y_pred = self.classes_.take(d.argmax(axis=1))
        return y_pred

    def predict_proba(self, X):
        """
        Return posterior probabilities of classification

        :param X: ndarray of shape (n_samples, n_features)
            Test data

        :return: ndarray of shape (n_samples, n_classes)
            Posterior probabilities of classification per class.
        """
        values = self._decision_function(X)
        # Compute likelihood of underlying Gaussian models
        likelihood = np.exp(values - values.max(axis=1)[:, np.newaxis])
        # Compute posterior probabilities
        return likelihood / likelihood.sum(axis=1)[:, np.newaxis]

    def predict_log_proba(self, X):
        """
        Return log of posterior probabilities of classification.

        :param X: ndarray of shape (n_samples, n_features)
            Test data

        :return: ndarray of shape (n_samples, n_classes)
            Posterior log-probabilities of classification per class.
        """
        probas_ = self.predict_proba(X)
        return np.log(probas_)
